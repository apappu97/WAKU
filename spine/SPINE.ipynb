{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SPINE.ipynb","provenance":[],"collapsed_sections":["Yai6N1F7LYTP","BOTWhANzLiY-","dk-9v82JZgVF","tZtX8aNEeWfy","W0X6ZU1TZeiI","T68mLNoZet_D","TbyhcFmV4oLX","yaXHVR27Moeq","6EYSnL-VZZ_S","xlGfSWD-RJ9p","W96b3mzzCdqW"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZzX2_MEZHrOu","colab_type":"text"},"source":["**Notebook structure**\n","\n","This notebook comprises three sections:\n","\n","*0 - Importing libraries, mounting Google drive and defining paths*\n","\n","*1 - Defining SPINE classes and functions*\n","\n","*2 - Execution*\n","\n","Execution is divided up into:\n","\n","*2a - Grid-search*: runs grid-search then writes a file search_300.npz containing grid-search results\n","\n","*2b - Tuple selection*: reads in search_300.npz then selects final hyperparameter tuples; stores tuple information in params_300.npz\n","\n","*2c - Embedding generation*: reads in params_300.npz and generates SPINE embeddings along with metadata file\n","\n","**Note**: lines which write files (to write search_300.npz, params_300.npz, SPINE embeddings or embeddings metadata) are presently commented out. This is done to ensure no overwriting of original WAKU versions of these files\n","\n","$\\color{red}{\\text{Please check the paths are defined correctly in section 0 before running this code}}$"]},{"cell_type":"markdown","metadata":{"id":"OqV-8jcR8Gyq","colab_type":"text"},"source":["**Sources**\n","\n","The code in this notebook is based on code from the following public repo: https://github.com/jacobdanovitch/SPINE\n","\n","The SPINE paper (Subramanian et al., 2018) can be found here: https://arxiv.org/pdf/1711.08792.pdf"]},{"cell_type":"markdown","metadata":{"id":"Yai6N1F7LYTP","colab_type":"text"},"source":["# 0. Importing libraries, mounting Google drive and defining paths"]},{"cell_type":"code","metadata":{"id":"2UjbsPDeZj5a","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import time\n","import pickle\n","import scipy\n","\n","from torch import nn\n","from random import shuffle\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import train_test_split\n","from scipy.stats import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uGwiQydpcEFu","colab_type":"code","colab":{}},"source":["# mount Google drive\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJ6uLftDdZ3O","colab_type":"code","colab":{}},"source":["# find WAKU on gdrive and cd into it\n","%cd '/content/gdrive'\n","directory_name = !find . -type d -name \"WAKU\"\n","directory = directory_name[0]\n","print(directory)\n","%cd $directory"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ofDVYaCoUTSZ","colab_type":"code","colab":{}},"source":["### paths\n","\n","# dense embeddings and corresponding dict\n","path_dense = './embeddings/dense/300_0.0_embeddings.npz'\n","path_dict = './embeddings/index2word.pickle'\n","\n","# WordSim-353 dataset\n","path_ws = './raw_data/word_similarity/WordSim353.txt'\n","\n","# search_300.npz and params_300.npz\n","path_search_300 = './spine/search_300.npz'\n","path_params_300 = './spine/params_300.npz'\n","\n","# final embeddings and metadata file\n","path_embeddings = './embeddings/spine/sp_300_%dpercent_embeddings.npz'# %d in place of target sparsity\n","path_metadata = './embeddings/spine/sp_300_metadata.pickle'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BOTWhANzLiY-","colab_type":"text"},"source":["# 1. Defining SPINE classes and functions"]},{"cell_type":"markdown","metadata":{"id":"dk-9v82JZgVF","colab_type":"text"},"source":["## 1a. Helper functions"]},{"cell_type":"code","metadata":{"id":"dFdDSGfYkjuK","colab_type":"code","colab":{}},"source":["# pickle read and write functions\n","\n","def load_pickle(filepath):\n","    \"read in pickle file\"\n","    pickle_in = open(filepath,\"rb\")\n","    emb_dict = pickle.load(pickle_in)\n","    return emb_dict\n","\n","def save_pickle(d_object, filename):\n","    \"dump to pickle file\"\n","    with open(filename, 'wb') as f:\n","      pickle.dump(d_object, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-lQ0XVpgju5a","colab_type":"code","colab":{}},"source":["# txt read and write functions\n","\n","def loadData(filepath):\n","    \"loads SPINE style embeddings txt, returns data and words\"\n","    lines = open(filepath).readlines()\n","    data = []\n","    words = []\n","    for line in lines:\n","        tokens = line.strip().split()\n","        words.append(tokens[0])\n","        data.append([float(i) for i in tokens[1:]])\n","    data = np.array(data)\n","    return data, words\n","\n","def dump_vectors(X, outfile, words):\n","\t\"takes array X, list of words and writes to txt file\"\n","\tprint (\"shape\", X.shape)\n","\tassert len(X) == len(words)\n","\tfw = open(outfile, 'w')\n","\tfor i in range(len(words)):\n","\t\tfw.write(words[i] + \" \")\n","\t\tfor j in X[i]:\n","\t\t\tfw.write(str(j) + \" \")\n","\t\tfw.write(\"\\n\")\n","\tfw.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5uLNl2UKeEys","colab_type":"code","colab":{}},"source":["def compute_sparsity(X):\n","\t\"returns percentage of elements of X equal to zero (returns scalar regardless of dimensions of X)\"\n","\tnon_zeros = 1. * np.count_nonzero(X) # count number of non zero elements in X\n","\ttotal = X.size\n","\tsparsity = 100. * (1 - (non_zeros)/total)\n","\treturn sparsity\n","\n","def get_noise_features(n_samples, n_features, noise_amount):\n","\t\"generates n_samples X n_features array of gaussian noise with mean = 0, std = noise_amount\"\n","\tnoise_x,  _ =  make_blobs(n_samples=n_samples, n_features=n_features, cluster_std=noise_amount, centers=np.array([np.zeros(n_features)]))\n","\treturn noise_x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tZtX8aNEeWfy","colab_type":"text"},"source":["## 1b. DataHandler"]},{"cell_type":"code","metadata":{"id":"GrNZRQiIZjZa","colab_type":"code","colab":{}},"source":["class DataHandler:\n","\n","\tdef __init__(self):\n","\t\tpass\n","\n","\tdef loadData(self, data, words):\n","\t\tself.words = words\n","\t\tself.data = data\n","\t\tself.data_size = self.data.shape[0] # data_size = number of words\n","\t\tself.inp_dim = self.data.shape[1] # number of elements in each embedding (e.g. 300 for standard word2vec)\n","\t\tself.original_data = self.data[:] # store of original array\n","\n","\tdef getWordsList(self): # returns list of words\n","\t\treturn self.words\n","\n","\tdef getDataShape(self): # returns shape of data (words X elements array)\n","\t\treturn self.data.shape\n","\n","\tdef resetDataOrder(self): # resets order of data array (so matches self.words, list of words)\n","\t\tself.data = self.original_data[:]\n","\n","\tdef getNumberOfBatches(self, batch_size): # returns integer number of batches (including possible incomplete/remainder batch at end)\n","\t\treturn int(( self.data_size + batch_size - 1 ) / batch_size)\n","\n","\tdef getBatch(self, i, batch_size, noise_level, denoising):\n","\t\t\"returns input and target batch, adding noise to input if denoising = True\"\n","\t\tbatch_y = self.data[i*batch_size:min((i+1)*batch_size, self.data_size)] # returns i-th batch from rows of self.data\n","\t\tbatch_x = batch_y # replicates\n","\t\tif denoising:\n","\t\t\tbatch_x = batch_y + get_noise_features(batch_y.shape[0], self.inp_dim, noise_level) # if denoising True, applies noise to Y\n","\t\treturn batch_x, batch_y\n","\n","\tdef shuffleTrain(self):\n","\t\t\"returns shuffled version of data array\"\n","\t\tindices = np.arange(self.data_size)\n","\t\tnp.random.shuffle(indices)\n","\t\tself.data = self.data[indices]\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W0X6ZU1TZeiI","colab_type":"text"},"source":["## 1c. SPINEModel"]},{"cell_type":"code","metadata":{"id":"cfeLV2RYZfqu","colab_type":"code","colab":{}},"source":["class SPINEModel(torch.nn.Module):\n","\n","\tdef __init__(self, params):\n","\t\tsuper(SPINEModel, self).__init__()\n","\t\t\n","\t\t# params is argument of the model; a dictionary\n","\t\tself.inp_dim = params['inp_dim'] # size of input embeddings\n","\t\tself.hdim = params['hdim'] # size of hidden layer (output embeddings)\n","\t\tself.noise_level = params['noise_level'] # noise level (std of gaussian noise applied to inputs)\n","\t\tself.getReconstructionLoss = nn.MSELoss() # reconstruction loss set to MSE\n","\t\tself.rho_star = params['rho_star'] # rho_star (sparsity fraction)\n","\t\t\n","\t\t# autoencoder\n","\t\tself.linear1 = nn.Linear(self.inp_dim, self.hdim)\n","\t\t#print(self.linear1.weight.data)\n","\t\tself.linear2 = nn.Linear(self.hdim, self.inp_dim)\n","\t\t\n","\n","\tdef forward(self, batch_x, batch_y):\n","\t\t\n","\t\t# forward\n","\t\tlinear1_out = self.linear1(batch_x)\n","\t\th = linear1_out.clamp(min=0, max=1) # capped relu\n","\t\tout = self.linear2(h)\n","\n","\t\t# different terms of the loss\n","\t\tbatch_size = batch_x.size(0)\n","\t\treconstruction_loss = self.getReconstructionLoss(out, batch_y) # reconstruction loss\n","\t\tpsl_loss = self._getPSLLoss(h, batch_size) # partial sparsity loss\n","\t\tasl_loss = self._getASLLoss(h) # average sparsity loss\n","\t\ttotal_loss = reconstruction_loss + params['psl_coeff']*psl_loss + params['asl_coeff']*asl_loss\n","\t\t\n","\t\treturn out, h, total_loss, [reconstruction_loss, psl_loss, asl_loss]\n","\n","\n","\tdef _getPSLLoss(self, h, batch_size):\n","\t\t\"Computes PSL: Z * (1 - Z), averaged across data and hidden dimension\"\n","\t\treturn torch.sum(h*(1-h))/ (batch_size * self.hdim)\n","\n","\n","\tdef _getASLLoss(self, h):\n","\t\t\"Computes ASL: encourages mean activation of each hidden unit to be ~0.15\"\n","\t\t# h is a data points x hidden units array\n","\t\ttemp = (torch.mean(h, dim=0) - self.rho_star) # compute mean activation of each unit in hidden layer and subtract rho_star\n","\t\tif params['max'] == True:\n","\t\t\ttemp = temp.clamp(min=0) # take max of 0, temp\n","\t\treturn torch.sum(temp * temp) / self.hdim # MSE"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T68mLNoZet_D","colab_type":"text"},"source":["## 1d. Solver"]},{"cell_type":"code","metadata":{"id":"PSyh_hSBY5We","colab_type":"code","colab":{}},"source":["class Solver:\n","\n","\tdef __init__(self, params):\n","\n","\t\t# Build data handler\n","\t\tself.data_handler = DataHandler() # instantiate DataHandler\n","\t\tself.data_handler.loadData(params['data_train'], params['words_train']) # load data to create DataHandler.data and DataHandler.words\n","\t\tparams['inp_dim'] = self.data_handler.getDataShape()[1]\n","\n","\t\t# Build model\n","\t\tself.model = SPINEModel(params) # instantiate SPINEModel\n","\t\tself.dtype = torch.FloatTensor\n","\t\tuse_cuda = torch.cuda.is_available()\n","\t\tif use_cuda: # put data and model on GPU if available\n","\t\t\tself.model.cuda()\n","\t\t\tself.dtype = torch.cuda.FloatTensor\n","\t\tself.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.1) # SGD optimiser with lr=0.1\n","\n","\tdef train(self, params):\n","\t\tnum_epochs, batch_size = params['num_epochs'], params['batch_size'],\n","\t\toptimizer = self.optimizer\n","\t\tdtype = self.dtype\n","\t\ttrain_results = np.zeros((num_epochs,3))\n","\n","\t\t# cycle through epochs\n","\t\tfor iteration in range(num_epochs):\n","\t\t\tself.data_handler.shuffleTrain() # returns shuffled version of data array\n","\t\t\tnum_batches = self.data_handler.getNumberOfBatches(batch_size) # calculate number of batches\n","\t\t\tepoch_losses = np.zeros(4) # rl, asl, psl, total\n","\n","\t\t\t# cycle through batches\n","\t\t\ttotal_data = 0\n","\t\t\tfor batch_idx in range(num_batches):\n","\t\t\t\toptimizer.zero_grad() # clear old gradients\n","\t\t\t\tbatch_x, batch_y = self.data_handler.getBatch(batch_idx, batch_size, params['noise_level'], params['denoising']) # get batch data\n","\t\t\t\tbatch_x = torch.from_numpy(batch_x).type(dtype) # convert to tensor\n","\t\t\t\tbatch_y = torch.from_numpy(batch_y).type(dtype) # convert to tensor\n","\t\t\t\tout, h, loss, loss_terms = self.model(batch_x, batch_y) # run model\n","\t\t\t\treconstruction_loss, psl_loss, asl_loss = loss_terms\n","\t\t\t\tloss.backward() # compute gradients\n","\t\t\t\toptimizer.step() # parameter update\n","\n","\t\t\t\t# add to epoch losses\n","\t\t\t\tthis_size = batch_x.size(0)\n","\t\t\t\tepoch_losses[0]+=reconstruction_loss.item()*this_size\n","\t\t\t\tepoch_losses[1]+=asl_loss.item()*this_size\n","\t\t\t\tepoch_losses[2]+=psl_loss.item()*this_size\n","\t\t\t\tepoch_losses[3]+=loss.item()*this_size\n","\t\t\t\ttotal_data+=this_size\n","\n","\t\t\t# divide epoch losses by total data points\n","\t\t\tepoch_losses/=total_data\n","\n","\t\t\tfull_h = self.getSpineEmbeddings(params)\n","\t\t\tepoch_sparsity = compute_sparsity(full_h)\n","\t\t\tepoch_sim = sim(full_h)\n","\t \n","\t\t\t# fill epoch_results with sparsity, sim result and epoch number\n","\t\t\ttrain_results[iteration] = np.array([epoch_sparsity, epoch_sim, iteration+1])\n","\n","\t\t\t#print(\"After epoch %2r, RL = %.4f, ASL = %.4f, PSL = %.4f, total = %.4f, sparsity = %.4f, sim = %.4f\" %(iteration+1, epoch_losses[0], epoch_losses[1], epoch_losses[2], epoch_losses[3], epoch_sparsity, epoch_sim))\n","\n","\t\t# print results after convergence\n","\t\tprint(\"Stopped at epoch %2r, RL = %.4f, ASL = %.4f, PSL = %.4f, total = %.4f, sparsity = %.4f, sim = %.4f\" %(iteration+1, epoch_losses[0], epoch_losses[1], epoch_losses[2], epoch_losses[3], epoch_sparsity, epoch_sim))\n","\t\treturn train_results, full_h\n","\n","\n","\tdef getSpineEmbeddings(self, params):\n","\t\t\"returns numpy array of new embeddings (in original order)\"\n","\t\tret = []\n","\t\tself.data_handler.resetDataOrder()\n","\t\tnum_batches = self.data_handler.getNumberOfBatches(params['batch_size_eval'])\n","\t\tfor batch_idx in range(num_batches):\n","\t\t\tbatch_x, batch_y = self.data_handler.getBatch(batch_idx, params['batch_size_eval'], params['noise_level'], False)\n","\t\t\tbatch_x = torch.from_numpy(batch_x).type(self.dtype)\n","\t\t\tbatch_y = torch.from_numpy(batch_y).type(self.dtype)\n","\t\t\twith torch.no_grad():\n","\t\t\t\t_, h, _, _ = self.model(batch_x, batch_y)\n","\t\t\t\tret.extend(h.cpu().data.numpy())\n","\t\treturn np.array(ret)\n","\n","\tdef getWordsList(self):\n","\t\treturn self.data_handler.getWordsList()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TbyhcFmV4oLX","colab_type":"text"},"source":["## 1e. Word sim"]},{"cell_type":"code","metadata":{"id":"aEtI4hGm4m90","colab_type":"code","colab":{}},"source":["### load in word sim data\n","def loadTestData():\n","  \"loads in annotated pairs\"\n","  data = {}\n","  tmp = open(path_ws).readlines()\n","  data['words'] = [ row.strip().split('\\t')[0:2] for i, row in enumerate(tmp) if i!=0 ]\n","  data['sim_scores'] = [ float(row.strip().split('\\t')[2]) for i, row in enumerate(tmp) if i!=0 ]\n","  return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eTbLv0iC8bed","colab_type":"code","colab":{}},"source":["### main sim functions\n","def getSimilarity(e1, e2):\n","  \"computes cosine similarity (cosine of angle between embedding vectors)\"\n","  if ( np.sqrt(np.sum(e1*e1)) * np.sqrt(np.sum(e2*e2))) == 0:\n","    return -1\n","  else:\n","    return np.sum(e1 * e2)/( np.sqrt(np.sum(e1*e1)) * np.sqrt(np.sum(e2*e2)))\n"," \n","def getSimilarityScoreForWords(w1,w2, embeddings):\n","  if (w2 not in params['words_train']) or (w1 not in params['words_train']):\n","    return -1\n","  else:\n","    finalVector_w1 = embeddings[params['words_train'].index(w1)]\n","    finalVector_w2 = embeddings[params['words_train'].index(w2)]\n","    return getSimilarity(finalVector_w1, finalVector_w2)\n","\n","def sim(embeddings):\n","  pred_scores = []\n","  pred_scores = [[getSimilarityScoreForWords(w1w2[0],w1w2[1], embeddings), human_score] for w1w2, human_score in zip(sim_output_val['words'], sim_output_val['sim_scores'])]\n","  pred_scores = np.array( [ val for val in pred_scores if val[0] != -1])\n","  if len(pred_scores) > 316:\n","    spearman_rank_coeff, _ = spearmanr(pred_scores[:,0], pred_scores[:,1])\n","    return spearman_rank_coeff\n","  else:\n","    return np.nan\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yaXHVR27Moeq","colab_type":"text"},"source":["# 2. Execution"]},{"cell_type":"markdown","metadata":{"id":"6EYSnL-VZZ_S","colab_type":"text"},"source":["## 2a. Grid-search"]},{"cell_type":"markdown","metadata":{"id":"WVofmE3oVR2s","colab_type":"text"},"source":["read in word sim annotated pairs"]},{"cell_type":"code","metadata":{"id":"orzPS5ZMVQdu","colab_type":"code","colab":{}},"source":["data = loadTestData()\n","sim_output_val = {'sim_scores': data['sim_scores'], 'words': data['words']}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vHkA-J1E0dCc","colab_type":"text"},"source":["read in the dense embeddings (word2vec)"]},{"cell_type":"code","metadata":{"id":"gGmaU8qS3cxq","colab_type":"code","colab":{}},"source":["data_full = np.load(path_dense)['a']\n","words_full = load_pickle(path_dict)\n","words_full = list(words_full.values())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3fATailV6Cnl","colab_type":"text"},"source":["params"]},{"cell_type":"code","metadata":{"id":"vOp5hS1haEeR","colab_type":"code","colab":{}},"source":["params = {}\n","\n","# process params\n","params['hdim'] = 300\n","params['rho_star'] = 0.15\n","params['noise_level'] = 0.2\n","params['num_epochs'] = 30\n","params['batch_size'] = 64\n","params['batch_size_eval'] = 512\n","params['max'] = True\n","params['denoising'] = True\n","\n","# data params\n","params['data_train'] = data_full\n","params['words_train'] = words_full\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8DzQQxPeZqjH","colab_type":"text"},"source":["grid search"]},{"cell_type":"code","metadata":{"id":"e88MwRhjwtz1","colab_type":"code","colab":{}},"source":["# grid search\n","\n","search_300 = np.zeros((1,5))\n","for asl_coeff in [10, 1, 0.1, 0.01, 0.001]:\n","  for psl_coeff in [50, 20, 10, 4, 3, 2, 1, 0.75, 0.1, 0.01, 0.001]:\n","    \n","    # assign params\n","    params['asl_coeff'] = asl_coeff\n","    params['psl_coeff'] = psl_coeff\n","    \n","    # seeds\n","    torch.manual_seed(0) # fix torch seed for weight initialisation\n","    np.random.seed(0) # fix np seed for gaussian noise and shuffle during training\n","    \n","    # instantiate and train model\n","    solver = Solver(params)\n","    train_results, _ = solver.train(params)\n","\n","    # add columns for hyperparams\n","    train_results = np.insert(train_results,3,asl_coeff,axis=1)\n","    train_results = np.insert(train_results,4,psl_coeff,axis=1)\n","    \n","    # add to main table\n","    search_300 = np.append(search_300,train_results,axis=0)\n","    print('done asl =', asl_coeff, 'psl =', psl_coeff)\n","\n","# delete step row and save down\n","search_300 = np.delete(search_300, 0, 0)\n","#np.savez(path_search_300, search_300)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xlGfSWD-RJ9p","colab_type":"text"},"source":["## 2b. Tuple selection"]},{"cell_type":"code","metadata":{"id":"FzEzuoY1QSxW","colab_type":"code","colab":{}},"source":["#### define epoch floor ####\n","e_floor = 10\n","search_300 = np.load(path_search_300)['arr_0']\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HmD_DM6cQVfs","colab_type":"code","colab":{}},"source":["##### select optimal hyperparameters\n","\n","targets = [30, 50, 70, 90]\n","params_300 = np.zeros((len(targets),6))\n","\n","search_300_sorted = search_300[(-1*search_300[:,1]).argsort()]\n","search_300_sorted = search_300_sorted[search_300_sorted[:,2]>=e_floor]\n","\n","for t, target in enumerate(targets):\n","  idx = (search_300_sorted[:,0]>target-2.5) & (search_300_sorted[:,0]<target+2.5)\n","  print('%d percent target' % target)\n","  print('actual sparsity =',search_300_sorted[idx][0,0])\n","  print('sim =',search_300_sorted[idx][0,1])\n","  print('epoch, asl, psl =', search_300_sorted[idx][0,2:])\n","  print('\\n')\n","  params_300[t,0:5] = search_300_sorted[idx][0]\n","  params_300[t,5] = target\n","\n","#np.savez(path_params_300, params_300)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oEPHCHuMQbpl","colab_type":"code","colab":{}},"source":["# load in search data and create beam chart\n","\n","filter_300 = search_300[search_300[:,2]>=e_floor]\n","plt.figure(figsize=(12,7))\n","plt.axvspan(7.5, 12.5, color='lightgrey',alpha=0.4)\n","plt.axvspan(27.5, 32.5, color='lightgrey',alpha=0.4)\n","plt.axvspan(47.5, 52.5, color='lightgrey',alpha=0.4)\n","plt.axvspan(67.5, 72.5, color='lightgrey',alpha=0.4)\n","plt.axvspan(87.5, 92.5, color='lightgrey',alpha=0.4)\n","plt.scatter(filter_300[:,0],filter_300[:,1],s=10)\n","\n","plt.xticks(np.arange(0, 110, step=10))\n","plt.ylim(top=0.395)\n","plt.xlim(20,100)\n","\n","plt.ylabel(r'word similarity score, $\\rho_{sim}$',size=20)\n","plt.xlabel(r'sparsity (%)',size=20)\n","plt.title('Word similarity vs. sparsity following SPINE grid search \\n',size=20)\n","\n","select = plt.scatter(params_300[:,0],params_300[:,1],c='r',marker='X',s=75)\n","\n","plt.tick_params(axis='both', which='major', labelsize=15)\n","plt.legend([select], ['tuples selected for downstream tasks'],loc=3, prop={'size': 15});"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W96b3mzzCdqW","colab_type":"text"},"source":["## 2c. Embedding generation"]},{"cell_type":"code","metadata":{"id":"eHxEN4fWEJyy","colab_type":"code","colab":{}},"source":["## params_300 has the following columns:\n","# target sparsity\n","# actual sparsity\n","# val sim score\n","# epoch number\n","# asl coeff\n","# psl coeff\n","\n","# read in npz and start meta-data dict\n","params_300 = np.load(path_params_300)['arr_0']\n","sp_300_metadata = {}\n","\n","# cycle through the rows\n","for row in params_300:\n","  \n","  # set params\n","  sparsity = row[0]\n","  val_sim = row[1]\n","  params['num_epochs'] = int(row[2]) # num epochs\n","  params['asl_coeff'] = row[3] # asl\n","  params['psl_coeff'] = row[4] # psl\n","  target = row[5]\n","\n","  # train the model\n","  torch.manual_seed(0)\n","  np.random.seed(0)\n","  solver = Solver(params)\n","  _, full_h = solver.train(params)\n","\n","  # save down embeddings\n","  #np.savez(path_embeddings % target, full_h)\n","\n","  # make row in dictionary\n","  sp_300_metadata['sp_300_%dpercent_embeddings' % target] = {'sparsity': sparsity, 'val_sim': val_sim, 'num_epochs': params['num_epochs'], 'asl_coeff': params['asl_coeff'], 'psl_coeff': params['psl_coeff']}\n","\n","# save down dictionary\n","#save_pickle(sp_300_metadata, path_metadata)"],"execution_count":0,"outputs":[]}]}