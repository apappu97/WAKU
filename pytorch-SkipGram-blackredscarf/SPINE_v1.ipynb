{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SPINE_v1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMutpQr90P2JHFT0fJnoYab"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_aEuqHU5czcx","colab_type":"text"},"source":["**Questions**\n","\n","- What common file type should we used in between embedding models and downstream tests? txt would be more general / accessible..\n","- What should the stopping criterion for training be? How do we ensure that this means a fair comparison with the word2vec model?\n","- All hyperparameters set to tuned values stated in paper. Batch size and learning rate set to 'defaults'. Are we happy with this?\n","- Not sure about the way loss functions are summed up each epoch. In paper they're averaged across data points. In the code they're averaged across data points... then summed across batches. Should we leave this?\n","- Should we compare the actual sparsity of the embeddings out of all methods with a single metric? cf. sparsity metric in this script"]},{"cell_type":"code","metadata":{"id":"2UjbsPDeZj5a","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","import time\n","\n","from torch import nn\n","from random import shuffle\n","from sklearn.datasets import make_blobs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uGwiQydpcEFu","colab_type":"code","outputId":"2c5b1be0-0ad6-4897-f0a2-ba395b558451","executionInfo":{"status":"ok","timestamp":1582911508737,"user_tz":0,"elapsed":474,"user":{"displayName":"William Lamb","photoUrl":"","userId":"14469785810749920373"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# mount Google drive\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5AY4dfi9urFA","colab_type":"code","outputId":"3f81acdd-c6d9-4e83-d0cc-2b1bf607c113","executionInfo":{"status":"ok","timestamp":1582909614725,"user_tz":0,"elapsed":2383,"user":{"displayName":"William Lamb","photoUrl":"","userId":"14469785810749920373"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# test that mounting to drive works\n","\n","def loadData(filename):\n","  lines = open(filename).readlines() # opens file, reads lines\n","  data = []\n","  words = []\n","  for line in lines:\n","    tokens = line.strip().split()\n","    words.append(tokens[0]) # append word to DataHandler.words\n","    data.append([float(i) for i in tokens[1:]]) # append embedding vector to DataHandler.words\n","  data = np.array(data)\n","  return data\n","\n","data = loadData('/content/gdrive/My Drive/NLP Class/WGL/embeddings/word2vec_original_15k_300d_train.txt')\n","print(data.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(15000, 300)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dk-9v82JZgVF","colab_type":"text"},"source":["**utils**\n","- constructs DataHandler class to load, retrieve and process data\n","- defines 3 helper functions"]},{"cell_type":"code","metadata":{"id":"GrNZRQiIZjZa","colab_type":"code","colab":{}},"source":["### DataHandler ----------------------------------------------------------------\n","\n","class DataHandler:\n","\n","\tdef __init__(self):\n","\t\tpass\n","\n","\tdef loadData(self, filename):\n","\t\tlines = open(filename).readlines() # opens file, reads lines\n","\t\tself.data = []\n","\t\tself.words = []\n","\t\tfor line in lines:\n","\t\t\ttokens = line.strip().split()\n","\t\t\tself.words.append(tokens[0]) # append word to DataHandler.words\n","\t\t\tself.data.append([float(i) for i in tokens[1:]]) # append embedding vector to DataHandler.words\n","\t\tself.data = np.array(self.data)\n","\t\tself.data_size = self.data.shape[0] # data_size = number of words\n","\t\tself.inp_dim = self.data.shape[1] # number of elements in each embedding (e.g. 300 for standard word2vec)\n","\t\tself.original_data = self.data[:] # store of original array\n","\n","\tdef getWordsList(self): # returns list of words\n","\t\treturn self.words\n","\n","\tdef getDataShape(self): # returns shape of data (words X elements array)\n","\t\treturn self.data.shape\n","\n","\tdef resetDataOrder(self): # resets order of data array (so matches self.words, list of words)\n","\t\tself.data = self.original_data[:]\n","\n","\tdef getNumberOfBatches(self, batch_size): # returns integer number of batches (including possible incomplete/remainder batch at end)\n","\t\treturn int(( self.data_size + batch_size - 1 ) / batch_size)\n","\n","\tdef getBatch(self, i, batch_size, noise_level, denoising):\n","\t\t\"returns input and target batch, adding noise to input if denoising = True\"\n","\t\tbatch_y = self.data[i*batch_size:min((i+1)*batch_size, self.data_size)] # returns i-th batch from rows of self.data\n","\t\tbatch_x = batch_y # replicates\n","\t\tif denoising:\n","\t\t\tbatch_x = batch_y + get_noise_features(batch_y.shape[0], self.inp_dim, noise_level) # if denoising True, applies noise to Y\n","\t\treturn batch_x, batch_y\n","\n","\tdef shuffleTrain(self):\n","\t\t\"returns shuffled version of data array\"\n","\t\tindices = np.arange(self.data_size)\n","\t\tnp.random.shuffle(indices)\n","\t\tself.data = self.data[indices]\n","\n","### Helper functions -----------------------------------------------------------\n","\n","def compute_sparsity(X):\n","\t\"returns percentage of elements of X equal to zero (returns scalar regardless of dimensions of X)\"\n","\tnon_zeros = 1. * np.count_nonzero(X) # count number of non zero elements in X\n","\ttotal = X.size\n","\tsparsity = 100. * (1 - (non_zeros)/total)\n","\treturn sparsity\n","\n","def dump_vectors(X, outfile, words):\n","\t\"takes array X, list of words and writes to txt file\"\n","\tprint (\"shape\", X.shape)\n","\tassert len(X) == len(words) #TODO print error statement\n","\tfw = open(outfile, 'w') # open outfile\n","\tfor i in range(len(words)):\n","\t\tfw.write(words[i] + \" \")\n","\t\tfor j in X[i]:\n","\t\t\tfw.write(str(j) + \" \")\n","\t\tfw.write(\"\\n\")\n","\tfw.close()\n","\n","def get_noise_features(n_samples, n_features, noise_amount):\n","\t\"generates n_samples X n_features array of gaussian noise with mean = 0, std = noise_amount\"\n","\tnoise_x,  _ =  make_blobs(n_samples=n_samples, n_features=n_features, # n_samples = number of words; n_features embedding lengths\n","\t\t\tcluster_std=noise_amount,\n","\t\t\tcenters=np.array([np.zeros(n_features)]))\n","\treturn noise_x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W0X6ZU1TZeiI","colab_type":"text"},"source":["**model**\n","\n","SPINE autoencoder class\n","- input is (batch_size x original embedding size) tensor\n","- h is (batch_size x hidden layer size) tensor\n","- out is (batch_size x original embedding size) tensor\n","\n","Note:\n","- PSL and ASL averaged rather than summed over hidden layer, contrary to paper; does these mean equations in paper are wrong?"]},{"cell_type":"code","metadata":{"id":"cfeLV2RYZfqu","colab_type":"code","colab":{}},"source":["class SPINEModel(torch.nn.Module):\n","\n","\tdef __init__(self, params):\n","\t\tsuper(SPINEModel, self).__init__()\n","\t\t\n","\t\t# params is argument of the model; a dictionary\n","\t\tself.inp_dim = params['inp_dim'] # size of input embeddings\n","\t\tself.hdim = params['hdim'] # size of hidden layer (output embeddings)\n","\t\tself.noise_level = params['noise_level'] # noise level (std of gaussian noise applied to inputs)\n","\t\tself.getReconstructionLoss = nn.MSELoss() # reconstruction loss set to MSE\n","\t\tself.rho_star = 1.0 - params['sparsity'] # rho_star (sparsity fraction)\n","\t\t\n","\t\t# autoencoder\n","\t\tself.linear1 = nn.Linear(self.inp_dim, self.hdim)\n","\t\tself.linear2 = nn.Linear(self.hdim, self.inp_dim)\n","\t\t\n","\n","\tdef forward(self, batch_x, batch_y):\n","\t\t\n","\t\t# forward\n","\t\tlinear1_out = self.linear1(batch_x)\n","\t\th = linear1_out.clamp(min=0, max=1) # capped relu\n","\t\tout = self.linear2(h)\n","\n","\t\t# different terms of the loss\n","\t\tbatch_size = batch_x.size(0)\n","\t\treconstruction_loss = self.getReconstructionLoss(out, batch_y) # reconstruction loss\n","\t\tpsl_loss = self._getPSLLoss(h, batch_size) # partial sparsity loss\n","\t\tasl_loss = self._getASLLoss(h) # average sparsity loss\n","\t\ttotal_loss = reconstruction_loss + psl_loss + asl_loss\n","\t\t\n","\t\treturn out, h, total_loss, [reconstruction_loss, psl_loss, asl_loss]\n","\n","\n","\tdef _getPSLLoss(self, h, batch_size):\n","\t\t\"Computes PSL: Z * (1 - Z), averaged across data and hidden dimension\"\n","\t\treturn torch.sum(h*(1-h))/ (batch_size * self.hdim)\n","\n","\n","\tdef _getASLLoss(self, h):\n","\t\t\"Computes ASL: encourages mean activation of each hidden unit to be ~0.15\"\n","\t\t# h is a data points x hidden units array\n","\t\ttemp = torch.mean(h, dim=0) - self.rho_star # compute mean activation of each unit in hidden layer and subtract rho_star\n","\t\ttemp = temp.clamp(min=0) # take max of 0, temp\n","\t\treturn torch.sum(temp * temp) / self.hdim # MSE"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6EYSnL-VZZ_S","colab_type":"text"},"source":["**main**\n","- define (hyper)parameters\n","- construct Solver class to train model and extract sparse embeddings from hidden layer\n","- train and output final embeddings"]},{"cell_type":"code","metadata":{"id":"vOp5hS1haEeR","colab_type":"code","colab":{}},"source":["params = {}\n","\n","# embedding size (paper = 1000)\n","params['hdim'] = 1000\n","\n","# add noise to inputs or not (paper = True)\n","params['denoising'] = True\n","\n","# noise level std (paper = 0.2)\n","params['noise_level'] = 0.2\n","\n","# sparsity; fraction of elements we want to be 0; feeds into rho_star (paper = 0.85)\n","params['sparsity'] = 0.85\n","\n","\n","\n","# number of epochs (default = 100)\n","params['num_epochs'] = 100\n","\n","# batch size (default = 64)\n","params['batch_size'] = 64\n","\n","\n","\n","# input and output files\n","params['input'] = '/content/gdrive/My Drive/NLP Class/WGL/embeddings/word2vec_original_15k_300d_train.txt'\n","params['output'] = '/content/gdrive/My Drive/NLP Class/WGL/embeddings/spine_test_out.txt'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PSyh_hSBY5We","colab_type":"code","colab":{}},"source":["class Solver:\n","\n","\tdef __init__(self, params):\n","\n","\t\t# Build data handler\n","\t\tself.data_handler = DataHandler() # instantiate DataHandler\n","\t\tself.data_handler.loadData(params['input']) # load data to create DataHandler.data and DataHandler.words\n","\t\tparams['inp_dim'] = self.data_handler.getDataShape()[1]\n","\n","\n","\t\t# Build model\n","\t\tself.model = SPINEModel(params) # instantiate SPINEModel\n","\t\tself.dtype = torch.FloatTensor\n","\t\tuse_cuda = torch.cuda.is_available()\n","\t\tif use_cuda: # put data and model on GPU if available\n","\t\t\tself.model.cuda()\n","\t\t\tself.dtype = torch.cuda.FloatTensor\n","\t\tself.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.1) # SGD optimiser with lr=0.1\n","\n","\n","\tdef train(self, params):\n","\t\tnum_epochs, batch_size = params['num_epochs'], params['batch_size'],\n","\t\toptimizer = self.optimizer\n","\t\tdtype = self.dtype\n","\n","\t\t# cycle through epochs\n","\t\tfor iteration in range(num_epochs):\n","\t\t\tself.data_handler.shuffleTrain() # returns shuffled version of data array\n","\t\t\tnum_batches = self.data_handler.getNumberOfBatches(batch_size) # calculate number of batches\n","\t\t\tepoch_losses = np.zeros(4) # rl, asl, psl, total\n","\n","\t\t\t# cycle through batches\n","\t\t\tfor batch_idx in range(num_batches):\n","\t\t\t\toptimizer.zero_grad() # clear old gradients\n","\t\t\t\tbatch_x, batch_y = self.data_handler.getBatch(batch_idx, batch_size, params['noise_level'], params['denoising'] ) # get batch data\n","\t\t\t\tbatch_x = torch.from_numpy(batch_x).type(dtype) # convert to tensor\n","\t\t\t\tbatch_y = torch.from_numpy(batch_y).type(dtype) # convert to tensor\n","\t\t\t\tout, h, loss, loss_terms = self.model(batch_x, batch_y) # run model\n","\t\t\t\treconstruction_loss, psl_loss, asl_loss = loss_terms\n","\t\t\t\tloss.backward() # compute gradients\n","\t\t\t\toptimizer.step() # parameter update\n","\n","\t\t\t\t# add to epoch losses\n","\t\t\t\tepoch_losses[0]+=reconstruction_loss.item()\n","\t\t\t\tepoch_losses[1]+=asl_loss.item()\n","\t\t\t\tepoch_losses[2]+=psl_loss.item()\n","\t\t\t\tepoch_losses[3]+=loss.item()\n","\t\t\n","\t\t\tprint(\"After epoch %r, Reconstruction Loss = %.4f, ASL = %.4f,\"\\\n","\t\t\t\t\t\t\"PSL = %.4f, and total = %.4f\"\n","\t\t\t\t\t\t%(iteration+1, epoch_losses[0], epoch_losses[1], epoch_losses[2], epoch_losses[3]) )\n","\t\t\t\n","\t\t\t# TODO: also print sparsity as with original code\n","\n","\tdef getSpineEmbeddings(self, batch_size, params):\n","\t\t\"returns numpy array of new embeddings (in original order)\"\n","\t\tret = []\n","\t\tself.data_handler.resetDataOrder()\n","\t\tnum_batches = self.data_handler.getNumberOfBatches(batch_size)\n","\t\tfor batch_idx in range(num_batches):\n","\t\t\tbatch_x, batch_y = self.data_handler.getBatch(batch_idx, batch_size, params['noise_level'], params['denoising'] )\n","\t\t\tbatch_x = torch.from_numpy(batch_x).type(self.dtype)\n","\t\t\tbatch_y = torch.from_numpy(batch_y).type(self.dtype)\n","\t\t\t_, h, _, _ = self.model(batch_x, batch_y)\n","\t\t\tret.extend(h.cpu().data.numpy())\n","\t\treturn np.array(ret)\n","\n","\tdef getWordsList(self):\n","\t\treturn self.data_handler.getWordsList()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bjjub2zZVc_9","colab_type":"code","outputId":"0344e4d8-1705-48a7-9767-7e786591882c","executionInfo":{"status":"ok","timestamp":1582910245631,"user_tz":0,"elapsed":75529,"user":{"displayName":"William Lamb","photoUrl":"","userId":"14469785810749920373"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["time1 = time.time()\n","\n","# instantiate Solver and train\n","solver = Solver(params)\n","solver.train(params)\n","\t\n","# output final embeddings\n","output_path = params['output']\n","final_batch_size = 512\n","spine_embeddings = solver.getSpineEmbeddings(final_batch_size, params) # (array output)\n","dump_vectors(spine_embeddings, output_path, solver.getWordsList()) # (dump to txt)\n","\n","time2 = time.time()\n","\n","print('time:',time2-time1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["After epoch 1, Reconstruction Loss = 7.7457, ASL = 0.0000,PSL = 11.1187, and total = 18.8644\n","After epoch 2, Reconstruction Loss = 7.3466, ASL = 0.0000,PSL = 9.9387, and total = 17.2853\n","After epoch 3, Reconstruction Loss = 7.2054, ASL = 0.0000,PSL = 8.9071, and total = 16.1126\n","After epoch 4, Reconstruction Loss = 7.1220, ASL = 0.0000,PSL = 7.9870, and total = 15.1090\n","After epoch 5, Reconstruction Loss = 7.0605, ASL = 0.0000,PSL = 7.1987, and total = 14.2593\n","After epoch 6, Reconstruction Loss = 7.0115, ASL = 0.0000,PSL = 6.4795, and total = 13.4910\n","After epoch 7, Reconstruction Loss = 6.9714, ASL = 0.0000,PSL = 5.8721, and total = 12.8434\n","After epoch 8, Reconstruction Loss = 6.9386, ASL = 0.0000,PSL = 5.3306, and total = 12.2693\n","After epoch 9, Reconstruction Loss = 6.9126, ASL = 0.0000,PSL = 4.8532, and total = 11.7658\n","After epoch 10, Reconstruction Loss = 6.8932, ASL = 0.0000,PSL = 4.4386, and total = 11.3318\n","After epoch 11, Reconstruction Loss = 6.8739, ASL = 0.0000,PSL = 4.0648, and total = 10.9387\n","After epoch 12, Reconstruction Loss = 6.8577, ASL = 0.0000,PSL = 3.7429, and total = 10.6006\n","After epoch 13, Reconstruction Loss = 6.8446, ASL = 0.0000,PSL = 3.4450, and total = 10.2896\n","After epoch 14, Reconstruction Loss = 6.8325, ASL = 0.0000,PSL = 3.1863, and total = 10.0188\n","After epoch 15, Reconstruction Loss = 6.8211, ASL = 0.0000,PSL = 2.9605, and total = 9.7817\n","After epoch 16, Reconstruction Loss = 6.8159, ASL = 0.0000,PSL = 2.7510, and total = 9.5669\n","After epoch 17, Reconstruction Loss = 6.8099, ASL = 0.0000,PSL = 2.5683, and total = 9.3783\n","After epoch 18, Reconstruction Loss = 6.8047, ASL = 0.0000,PSL = 2.4083, and total = 9.2129\n","After epoch 19, Reconstruction Loss = 6.7978, ASL = 0.0000,PSL = 2.2465, and total = 9.0443\n","After epoch 20, Reconstruction Loss = 6.7934, ASL = 0.0000,PSL = 2.1135, and total = 8.9069\n","After epoch 21, Reconstruction Loss = 6.7921, ASL = 0.0000,PSL = 1.9963, and total = 8.7884\n","After epoch 22, Reconstruction Loss = 6.7872, ASL = 0.0000,PSL = 1.8838, and total = 8.6709\n","After epoch 23, Reconstruction Loss = 6.7867, ASL = 0.0000,PSL = 1.7751, and total = 8.5618\n","After epoch 24, Reconstruction Loss = 6.7830, ASL = 0.0000,PSL = 1.6727, and total = 8.4557\n","After epoch 25, Reconstruction Loss = 6.7842, ASL = 0.0000,PSL = 1.5917, and total = 8.3759\n","After epoch 26, Reconstruction Loss = 6.7804, ASL = 0.0000,PSL = 1.5135, and total = 8.2940\n","After epoch 27, Reconstruction Loss = 6.7771, ASL = 0.0000,PSL = 1.4359, and total = 8.2130\n","After epoch 28, Reconstruction Loss = 6.7790, ASL = 0.0000,PSL = 1.3703, and total = 8.1493\n","After epoch 29, Reconstruction Loss = 6.7798, ASL = 0.0000,PSL = 1.3071, and total = 8.0869\n","After epoch 30, Reconstruction Loss = 6.7771, ASL = 0.0000,PSL = 1.2492, and total = 8.0263\n","After epoch 31, Reconstruction Loss = 6.7769, ASL = 0.0000,PSL = 1.1871, and total = 7.9640\n","After epoch 32, Reconstruction Loss = 6.7774, ASL = 0.0000,PSL = 1.1414, and total = 7.9188\n","After epoch 33, Reconstruction Loss = 6.7770, ASL = 0.0000,PSL = 1.0915, and total = 7.8685\n","After epoch 34, Reconstruction Loss = 6.7752, ASL = 0.0000,PSL = 1.0473, and total = 7.8225\n","After epoch 35, Reconstruction Loss = 6.7771, ASL = 0.0000,PSL = 1.0141, and total = 7.7911\n","After epoch 36, Reconstruction Loss = 6.7767, ASL = 0.0000,PSL = 0.9666, and total = 7.7433\n","After epoch 37, Reconstruction Loss = 6.7765, ASL = 0.0000,PSL = 0.9244, and total = 7.7009\n","After epoch 38, Reconstruction Loss = 6.7742, ASL = 0.0000,PSL = 0.8964, and total = 7.6707\n","After epoch 39, Reconstruction Loss = 6.7775, ASL = 0.0000,PSL = 0.8606, and total = 7.6381\n","After epoch 40, Reconstruction Loss = 6.7763, ASL = 0.0000,PSL = 0.8334, and total = 7.6096\n","After epoch 41, Reconstruction Loss = 6.7795, ASL = 0.0000,PSL = 0.8044, and total = 7.5839\n","After epoch 42, Reconstruction Loss = 6.7759, ASL = 0.0000,PSL = 0.7762, and total = 7.5522\n","After epoch 43, Reconstruction Loss = 6.7797, ASL = 0.0000,PSL = 0.7511, and total = 7.5308\n","After epoch 44, Reconstruction Loss = 6.7778, ASL = 0.0000,PSL = 0.7293, and total = 7.5071\n","After epoch 45, Reconstruction Loss = 6.7803, ASL = 0.0000,PSL = 0.7031, and total = 7.4834\n","After epoch 46, Reconstruction Loss = 6.7780, ASL = 0.0000,PSL = 0.6815, and total = 7.4595\n","After epoch 47, Reconstruction Loss = 6.7814, ASL = 0.0000,PSL = 0.6659, and total = 7.4473\n","After epoch 48, Reconstruction Loss = 6.7801, ASL = 0.0000,PSL = 0.6400, and total = 7.4201\n","After epoch 49, Reconstruction Loss = 6.7809, ASL = 0.0000,PSL = 0.6222, and total = 7.4031\n","After epoch 50, Reconstruction Loss = 6.7825, ASL = 0.0000,PSL = 0.6058, and total = 7.3883\n","After epoch 51, Reconstruction Loss = 6.7820, ASL = 0.0000,PSL = 0.5906, and total = 7.3726\n","After epoch 52, Reconstruction Loss = 6.7840, ASL = 0.0000,PSL = 0.5775, and total = 7.3615\n","After epoch 53, Reconstruction Loss = 6.7796, ASL = 0.0000,PSL = 0.5570, and total = 7.3366\n","After epoch 54, Reconstruction Loss = 6.7812, ASL = 0.0000,PSL = 0.5450, and total = 7.3262\n","After epoch 55, Reconstruction Loss = 6.7831, ASL = 0.0000,PSL = 0.5262, and total = 7.3094\n","After epoch 56, Reconstruction Loss = 6.7846, ASL = 0.0000,PSL = 0.5176, and total = 7.3022\n","After epoch 57, Reconstruction Loss = 6.7844, ASL = 0.0000,PSL = 0.5007, and total = 7.2851\n","After epoch 58, Reconstruction Loss = 6.7873, ASL = 0.0000,PSL = 0.4891, and total = 7.2765\n","After epoch 59, Reconstruction Loss = 6.7867, ASL = 0.0000,PSL = 0.4818, and total = 7.2685\n","After epoch 60, Reconstruction Loss = 6.7849, ASL = 0.0000,PSL = 0.4661, and total = 7.2510\n","After epoch 61, Reconstruction Loss = 6.7870, ASL = 0.0000,PSL = 0.4577, and total = 7.2447\n","After epoch 62, Reconstruction Loss = 6.7868, ASL = 0.0000,PSL = 0.4426, and total = 7.2294\n","After epoch 63, Reconstruction Loss = 6.7878, ASL = 0.0000,PSL = 0.4333, and total = 7.2211\n","After epoch 64, Reconstruction Loss = 6.7885, ASL = 0.0000,PSL = 0.4226, and total = 7.2111\n","After epoch 65, Reconstruction Loss = 6.7872, ASL = 0.0000,PSL = 0.4161, and total = 7.2034\n","After epoch 66, Reconstruction Loss = 6.7850, ASL = 0.0000,PSL = 0.4018, and total = 7.1868\n","After epoch 67, Reconstruction Loss = 6.7854, ASL = 0.0000,PSL = 0.3957, and total = 7.1810\n","After epoch 68, Reconstruction Loss = 6.7918, ASL = 0.0000,PSL = 0.3875, and total = 7.1793\n","After epoch 69, Reconstruction Loss = 6.7886, ASL = 0.0000,PSL = 0.3811, and total = 7.1697\n","After epoch 70, Reconstruction Loss = 6.7879, ASL = 0.0000,PSL = 0.3748, and total = 7.1627\n","After epoch 71, Reconstruction Loss = 6.7885, ASL = 0.0000,PSL = 0.3640, and total = 7.1525\n","After epoch 72, Reconstruction Loss = 6.7936, ASL = 0.0000,PSL = 0.3585, and total = 7.1521\n","After epoch 73, Reconstruction Loss = 6.7905, ASL = 0.0000,PSL = 0.3497, and total = 7.1403\n","After epoch 74, Reconstruction Loss = 6.7877, ASL = 0.0000,PSL = 0.3459, and total = 7.1336\n","After epoch 75, Reconstruction Loss = 6.7921, ASL = 0.0000,PSL = 0.3346, and total = 7.1267\n","After epoch 76, Reconstruction Loss = 6.7921, ASL = 0.0000,PSL = 0.3310, and total = 7.1230\n","After epoch 77, Reconstruction Loss = 6.7911, ASL = 0.0000,PSL = 0.3237, and total = 7.1148\n","After epoch 78, Reconstruction Loss = 6.7906, ASL = 0.0000,PSL = 0.3201, and total = 7.1108\n","After epoch 79, Reconstruction Loss = 6.7925, ASL = 0.0000,PSL = 0.3127, and total = 7.1052\n","After epoch 80, Reconstruction Loss = 6.7904, ASL = 0.0000,PSL = 0.3068, and total = 7.0972\n","After epoch 81, Reconstruction Loss = 6.7921, ASL = 0.0000,PSL = 0.3037, and total = 7.0957\n","After epoch 82, Reconstruction Loss = 6.7916, ASL = 0.0000,PSL = 0.2958, and total = 7.0874\n","After epoch 83, Reconstruction Loss = 6.7935, ASL = 0.0000,PSL = 0.2928, and total = 7.0862\n","After epoch 84, Reconstruction Loss = 6.7912, ASL = 0.0000,PSL = 0.2835, and total = 7.0748\n","After epoch 85, Reconstruction Loss = 6.7896, ASL = 0.0000,PSL = 0.2830, and total = 7.0726\n","After epoch 86, Reconstruction Loss = 6.7941, ASL = 0.0000,PSL = 0.2775, and total = 7.0716\n","After epoch 87, Reconstruction Loss = 6.7893, ASL = 0.0000,PSL = 0.2715, and total = 7.0608\n","After epoch 88, Reconstruction Loss = 6.7914, ASL = 0.0000,PSL = 0.2678, and total = 7.0592\n","After epoch 89, Reconstruction Loss = 6.7940, ASL = 0.0000,PSL = 0.2638, and total = 7.0579\n","After epoch 90, Reconstruction Loss = 6.7938, ASL = 0.0000,PSL = 0.2595, and total = 7.0533\n","After epoch 91, Reconstruction Loss = 6.7957, ASL = 0.0000,PSL = 0.2538, and total = 7.0495\n","After epoch 92, Reconstruction Loss = 6.7938, ASL = 0.0000,PSL = 0.2527, and total = 7.0465\n","After epoch 93, Reconstruction Loss = 6.7939, ASL = 0.0000,PSL = 0.2478, and total = 7.0417\n","After epoch 94, Reconstruction Loss = 6.7949, ASL = 0.0000,PSL = 0.2440, and total = 7.0389\n","After epoch 95, Reconstruction Loss = 6.7927, ASL = 0.0000,PSL = 0.2432, and total = 7.0359\n","After epoch 96, Reconstruction Loss = 6.7940, ASL = 0.0000,PSL = 0.2379, and total = 7.0319\n","After epoch 97, Reconstruction Loss = 6.7957, ASL = 0.0000,PSL = 0.2358, and total = 7.0315\n","After epoch 98, Reconstruction Loss = 6.7946, ASL = 0.0000,PSL = 0.2298, and total = 7.0244\n","After epoch 99, Reconstruction Loss = 6.7986, ASL = 0.0000,PSL = 0.2250, and total = 7.0236\n","After epoch 100, Reconstruction Loss = 6.7966, ASL = 0.0000,PSL = 0.2233, and total = 7.0199\n","shape (15000, 1000)\n","time: 75.01961350440979\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8DmlhViTxwOr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}