{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word_sim_v1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"_idwUi62PuNk","colab_type":"code","colab":{}},"source":["import numpy as np\n","import scipy\n","from scipy.stats import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jRbsnMAgMKlw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"bfc8c49f-fee0-458c-f806-e7ff0f8b9e1b","executionInfo":{"status":"ok","timestamp":1582918212240,"user_tz":0,"elapsed":496,"user":{"displayName":"William Lamb","photoUrl":"","userId":"14469785810749920373"}}},"source":["# mount Google drive\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IjD3jQmBPpNk","colab_type":"code","colab":{}},"source":["'''\n","annotated_pairs data set is from followinf paper:\n","Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: the concept revisited. In Proc. of WWW.\n","'''\n","\n","embeddings_source = '/content/gdrive/My Drive/NLP Class/WGL/embeddings/word2vec_original_15k_300d_train.txt'\n","annotated_pairs = '/content/gdrive/My Drive/NLP Class/WGL/human_sim.txt'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKe3M09mP-DC","colab_type":"code","colab":{}},"source":["##### 1) loading the data\n","\n","def loadData():\n","\t\"loads in the pre-trained embeddings; returns a dictionary mapping words to vectors\"\n","\tret = {}\n","\tdata = open(embeddings_source).readlines()\n","\tfor row in data:\n","\t\tword = row.strip().split(' ')[0]\n","\t\tvals = row.strip().split(' ')[1:]\n","\t\tvals = np.array( [float(val) for val in vals] )\n","\t\tret[word] = vals\n","\treturn ret\n","\n","def loadTestData():\n","  \"loads in annotated pairs\"\n","  data = {}\n","  tmp = open(annotated_pairs).readlines()\n","  data['words'] = [ row.strip().split('\\t')[0:2] for i, row in enumerate(tmp) if i!=0 ]\n","  data['sim_scores'] = [ float(row.strip().split('\\t')[2]) for i, row in enumerate(tmp) if i!=0 ]\n","  return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-DFP_dZRLJT","colab_type":"code","colab":{}},"source":["##### 2) getting similarity scores for word embeddings\n","\n","def getSimilarity(e1, e2):\n","\t\"computes cosine similarity (cosine of angle between embedding vectors)\"\n","\treturn np.sum(e1 * e2)/( np.sqrt(np.sum(e1*e1)) * np.sqrt(np.sum(e2*e2)))\n"," \n","def getSimilarityScoreForWords(w1,w2):\n","  global embeddings\n","  if (w2 not in embeddings) or (w1 not in embeddings):\n","    return -1\n","  else:\n","    finalVector_w1 = embeddings[w1]\n","    finalVector_w2 = embeddings[w2]\n","    return getSimilarity(finalVector_w1, finalVector_w2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"APbzGsySSS0s","colab_type":"code","colab":{}},"source":["##### 3) computing correlation between human-annotated scores and cosine similarities for word embeddings\n","\n","def evaluate():\n","  global embeddings\n","  embeddings = loadData()\n","  data = loadTestData()\n","  print(\"words = \", len(data['words']))\n","  print(\"scores = \", len(data['sim_scores']))\n","  pred_scores = []\n","  invalid = 0\n","\n","  # loop through human annotated data, returning matrix where col 0 is cosine similarity of embeddings, and col 1 is human score\n","  pred_scores = [[getSimilarityScoreForWords(w1w2[0],w1w2[1]), human_score] for w1w2, human_score in zip(data['words'], data['sim_scores'])]\n","\n","  # delete word pairs which couldn't be found in embedding set\n","  pred_scores = np.array( [ val for val in pred_scores if val[0] != -1])\n","\n","  spearman_rank_coeff, sp_rho = spearmanr(pred_scores[:,0], pred_scores[:,1])\n","  print(\"total, valid, spearman_rank_coeff, sp_rho\", len(data['words']),len(pred_scores), spearman_rank_coeff, sp_rho)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BmtlFTzLeyAN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":66},"outputId":"97367a6b-1f06-4c0a-a092-a8287ec3033e","executionInfo":{"status":"ok","timestamp":1582919173894,"user_tz":0,"elapsed":2417,"user":{"displayName":"William Lamb","photoUrl":"","userId":"14469785810749920373"}}},"source":["evaluate()"],"execution_count":47,"outputs":[{"output_type":"stream","text":["words =  353\n","scores =  353\n","total, valid, spearman_rank_coeff, sp_rho 353 292 0.6842646500002515 1.161001967489536e-41\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IJ_kudvaTgmS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}