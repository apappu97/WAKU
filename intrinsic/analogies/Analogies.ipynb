{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"name":"Analogies_local_practice.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vMy9biVUAuZJ","colab_type":"text"},"source":["## SETUP"]},{"cell_type":"markdown","metadata":{"id":"jLK9pmISAsQR","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"wRFnmYGW6106","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ce1f28cf-a3cc-4e4f-bcfb-6e02c13a1025","executionInfo":{"status":"ok","timestamp":1583084588321,"user_tz":0,"elapsed":478,"user":{"displayName":"Aneesh Pappu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAd7g9Mtf5hzYf5M_-AuNqtCn2V_uL1Ys5IGMC-sA=s64","userId":"12681851174811573325"}}},"source":["import requests\n","import gzip\n","import zipfile\n","import os\n","from os import path\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import copy\n","import re\n","import torch \n","import io\n","import time \n","import pickle\n","from collections import defaultdict\n","from datetime import datetime\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device used: \", device)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Device used:  cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1o2AU7TQ8Rdr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"1f6edc2d-8961-4803-f4f0-13d3721d8b3c","executionInfo":{"status":"ok","timestamp":1583082570390,"user_tz":0,"elapsed":109475,"user":{"displayName":"Aneesh Pappu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAd7g9Mtf5hzYf5M_-AuNqtCn2V_uL1Ys5IGMC-sA=s64","userId":"12681851174811573325"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GFzCdPWd8jyl","colab_type":"code","colab":{}},"source":["# if you want, you can use the below script to automatically find where the directory is in your GDrive and cd into it.\n","# %cd '/content/drive'\n","# directory_name = !find . -type d -name \"ucl-nlp-finalproject\"\n","# directory_name = directory_name[0]\n","# print(directory_name)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HwjOO4Pt8xcS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2c11fdb7-7eb9-458e-bfdf-c2e5755f2213","executionInfo":{"status":"ok","timestamp":1583082580170,"user_tz":0,"elapsed":479,"user":{"displayName":"Aneesh Pappu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAd7g9Mtf5hzYf5M_-AuNqtCn2V_uL1Ys5IGMC-sA=s64","userId":"12681851174811573325"}}},"source":["%cd 'drive/My Drive/Colab Notebooks'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"89SxZXtFNYvS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":181},"outputId":"ac574a86-1b68-400f-8e3d-f7362eb92dfb","executionInfo":{"status":"error","timestamp":1583084847318,"user_tz":0,"elapsed":529,"user":{"displayName":"Aneesh Pappu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAd7g9Mtf5hzYf5M_-AuNqtCn2V_uL1Ys5IGMC-sA=s64","userId":"12681851174811573325"}}},"source":["EXPERIMENT_FOLDER_NAME = \"\"\n","assert len(EXPERIMENT_FOLDER_NAME) != 0"],"execution_count":21,"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-b1a3052fd4f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEXPERIMENT_FOLDER_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXPERIMENT_FOLDER_NAME\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAssertionError\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"Q8JCAoojA-k_","colab_type":"code","colab":{}},"source":["## Load embeddings from .npy file and load W2V_dict (word to id) from pickle file\n","W2V_weights = np.load('embeddings.npy')\n","with open('word2index.pickle', 'rb') as handle:\n","  W2V_dict = pickle.load(handle)\n","# with open('word_freq.pickle', 'rb') as handle2:\n","#   W2freq_dict = pickle.load(handle2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J-9b1L5ZBqXG","colab_type":"code","colab":{}},"source":["# ## Sort by frequency to get top RESTRICT_NUM tokens -- standard seems to be top 300K.\n","# RESTRICT_NUM = 300000\n","\n","# ordered_freq_tuple_list = [(token, freq) for token, freq in zip(W2freq_dict.keys(), W2freq_dict.values())] # returns list of tuples [(token, freq)]\n","# ordered_freq_tuple_list = sorted(ordered_freq_tuple_list, key = lambda x: x[1], reverse=True) # sorts the above by the frequency in descending order \n","# restricted_freq_tuple_list = ordered_freq_tuple_list[:RESTRICT_NUM] # grab first RESTRICT_NUM tokens\n","\n","# # renumber the tokens\n","# W2V_dict = dict([(x[0], i) for i, x in enumerate(restricted_freq_tuple_list)])\n","\n","# # grab embeddings in this order \n","# indices_to_grab = [original_W2V_dict[token] for token, _ in restricted_freq_tuple_list]\n","# # create new matrix now \n","# W2V_weights = original_W2V_weights[indices_to_grab]\n","\n","# if original_W2V_weights.shape[0] < RESTRICT_NUM:\n","#   assert W2V_weights.shape[0] == original_W2V_weights.shape[0]\n","# else: \n","#   assert W2V_weights.shape[0] == RESTRICT_NUM"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KnuXLb8g611L","colab_type":"text"},"source":["### ANALOGIES"]},{"cell_type":"code","metadata":{"id":"azzwnJoY611M","colab_type":"code","colab":{}},"source":["# Credit to Facebook MUSE repo for base word analogy code -- didn't think to use matrix mult of normalized embeds to do cosine-distance nearest neighbor search, clever!\n","# I added gross CUDA code and dynamic sizing of matrices during batching as to not exceed Colab GPU memory allocation \n","\n","# IMPORTANT -- whether the model has embeddings only for lowercase. If so, lower should be True. If model has embeddings for uppercase (i.e. proper nouns), should be False.\n","\n","def get_word_id(word, word2id, lower):\n","    \"\"\"\n","    Get a word ID.\n","    If the model does not use lowercase and the evaluation file is lowercased,\n","    we might be able to find an associated word.\n","    \"\"\"\n","    assert type(lower) is bool\n","    word_id = word2id.get(word)\n","    if word_id is None and not lower:\n","        word_id = word2id.get(word.capitalize())\n","    if word_id is None and not lower:\n","        word_id = word2id.get(word.title())\n","    return word_id\n","\n","def get_wordanalogy_scores(dirpath, word2id, embeddings, lower):\n","    \"\"\"\n","    Return (english) word analogy score\n","    \"\"\"\n","    if not os.path.isdir(dirpath):\n","        return None\n","\n","    # normalize word embeddings\n","    embeddings = embeddings / np.sqrt((embeddings ** 2).sum(1))[:, None]\n","\n","    # scores by category\n","    scores = defaultdict(dict)\n","\n","    word_ids = {}\n","    queries = {}\n","\n","    num_examples_thrown = 0\n","    total_examples = 0\n","    with io.open(os.path.join(dirpath, 'questions-words.txt'), 'r', encoding='utf-8') as f:\n","        for line in f:\n","            # new line\n","            line = line.rstrip()\n","            if lower:\n","                line = line.lower()\n","\n","            # new category\n","            if \":\" in line:\n","                assert line[1] == ' '\n","                category = line[2:]\n","                assert category not in scores\n","                scores[category] = {'n_found': 0, 'n_not_found': 0, 'n_correct': 0}\n","                word_ids[category] = []\n","                queries[category] = []\n","                continue\n","\n","            # get word IDs\n","            assert len(line.split()) == 4, line\n","            word1, word2, word3, word4 = line.split()\n","\n","            word_id1 = get_word_id(word1, word2id, lower)\n","            word_id2 = get_word_id(word2, word2id, lower)\n","            word_id3 = get_word_id(word3, word2id, lower)\n","            word_id4 = get_word_id(word4, word2id, lower)\n","\n","            # if at least one word is not found\n","            if any(x is None for x in [word_id1, word_id2, word_id3, word_id4]):\n","                scores[category]['n_not_found'] += 1\n","                num_examples_thrown +=1 \n","                continue\n","            else:\n","                scores[category]['n_found'] += 1\n","                word_ids[category].append([word_id1, word_id2, word_id3, word_id4])\n","                # generate query vector and get nearest neighbors\n","                query = embeddings[word_id1] - embeddings[word_id2] + embeddings[word_id4]\n","                query = query / np.linalg.norm(query)\n","\n","                queries[category].append(query)\n","            total_examples += 1\n","    \n","    print(\"Done scanning, threw {} examples out of {} total = {} % discarded\".format(num_examples_thrown, total_examples, float(num_examples_thrown)/total_examples * 100))\n","    \n","    # Compute score for each category\n","    total_cats = len(queries)\n","    curr_cat = 0\n","    ROW_LIMIT = 500\n","\n","    with torch.no_grad(): # make sure to not store computational graph info\n","      for cat in queries:\n","\n","          start_time = time.time()\n","\n","          qs_np = np.vstack(queries[cat])\n","          qs_shape = qs_np.shape\n","          \n","          for i in range(0, qs_shape[0], ROW_LIMIT): #allocate matrices of size ROW LIMIT rows \n","            if i >= ROW_LIMIT:\n","              total_cats += 1 \n","            qs = torch.from_numpy(qs_np[i:i + ROW_LIMIT, :]).cuda()\n","            keys = torch.from_numpy(embeddings.T).cuda()\n","            values = qs.mm(keys)\n","\n","            # free up memory \n","            del qs\n","            del keys \n","            torch.cuda.empty_cache()\n","\n","            word_ids_tensor = torch.tensor(word_ids[cat]).cuda()\n","            curr_word_ids_tensor = word_ids_tensor[i:i+ROW_LIMIT, :]\n","\n","            # be sure we do not select input words\n","            for j, ws in enumerate(curr_word_ids_tensor):\n","                for wid in [ws[0], ws[1], ws[3]]:\n","                    values[j, wid] = -1e9\n","            maxes, indices = values.max(axis = 1)\n","            correct_indices = curr_word_ids_tensor[:, 2]\n","            num_correct = torch.sum(torch.eq(indices, correct_indices)).item()\n","            key = cat + \"_{}\".format(str(round(i/(ROW_LIMIT))))\n","            scores[key]['n_correct'] = num_correct\n","\n","            curr_cat +=1 \n","            print('finished batch {} out of {}, took {} seconds'.format(curr_cat, total_cats, time.time() - start_time))\n","            \n","            # clean up memory\n","            del values \n","            del word_ids_tensor\n","            del maxes\n","            del indices\n","            del correct_indices \n","            torch.cuda.empty_cache()\n","            # print(\"Current CUDA snapshot after del and empty cache at end of loop\", torch.cuda.memory_allocated())\n","\n","    # compute and log accuracies\n","\n","    print('computing total accuracy')\n","    total_correct = 0\n","    total_found = 0\n","\n","    for k in sorted(scores.keys()):\n","        v = scores[k]\n","        total_correct += v['n_correct']\n","        total_found += v.get('n_found', 0)\n","\n","    print(\"total correct: {}, total found: {}\".format(total_correct, total_found))\n","    total_accuracy = float(total_correct)/total_found\n","    print(\"total acc: {}\".format(total_accuracy))\n","    return scores, total_correct, total_found, total_accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LqcGAGqa1xRB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":867},"outputId":"c5e4eb5d-13d4-4af8-bb85-78305643f7b2","executionInfo":{"status":"ok","timestamp":1583084723848,"user_tz":0,"elapsed":20899,"user":{"displayName":"Aneesh Pappu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAd7g9Mtf5hzYf5M_-AuNqtCn2V_uL1Ys5IGMC-sA=s64","userId":"12681851174811573325"}}},"source":["# do it on the restricted weights, i.e. first 300000 word vectors (roughly sorted by frequency as per Mikolov's post)\n","start = time.time()\n","scores, total_correct, total_found, total_accuracy = get_wordanalogy_scores(\"./\", W2V_dict, W2V_weights, True) # False means we have embeddings for uppercased words, True menas embeddings for lowercase\n","end = time.time()\n","print(\"Completed, takes: {} seconds\".format(end - start))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Done scanning, threw 438 examples out of 19106 total = 2.292473568512509 % discarded\n","finished batch 1 out of 14, took 0.4430065155029297 seconds\n","finished batch 2 out of 15, took 0.7773175239562988 seconds\n","finished batch 3 out of 15, took 0.44916510581970215 seconds\n","finished batch 4 out of 16, took 0.8971123695373535 seconds\n","finished batch 5 out of 17, took 1.3446683883666992 seconds\n","finished batch 6 out of 18, took 1.7935545444488525 seconds\n","finished batch 7 out of 19, took 2.243464708328247 seconds\n","finished batch 8 out of 20, took 2.6937456130981445 seconds\n","finished batch 9 out of 21, took 3.1356089115142822 seconds\n","finished batch 10 out of 22, took 3.5828776359558105 seconds\n","finished batch 11 out of 23, took 3.9767773151397705 seconds\n","finished batch 12 out of 23, took 0.4341607093811035 seconds\n","finished batch 13 out of 24, took 0.8526771068572998 seconds\n","finished batch 14 out of 24, took 0.4484748840332031 seconds\n","finished batch 15 out of 25, took 0.9016714096069336 seconds\n","finished batch 16 out of 26, took 1.3558018207550049 seconds\n","finished batch 17 out of 27, took 1.8080108165740967 seconds\n","finished batch 18 out of 28, took 2.251357078552246 seconds\n","finished batch 19 out of 28, took 0.4427015781402588 seconds\n","finished batch 20 out of 29, took 0.7741703987121582 seconds\n","finished batch 21 out of 29, took 0.4347798824310303 seconds\n","finished batch 22 out of 30, took 0.8834049701690674 seconds\n","finished batch 23 out of 30, took 0.4459495544433594 seconds\n","finished batch 24 out of 31, took 0.8463034629821777 seconds\n","finished batch 25 out of 31, took 0.44410276412963867 seconds\n","finished batch 26 out of 32, took 0.8862447738647461 seconds\n","finished batch 27 out of 33, took 1.2932758331298828 seconds\n","finished batch 28 out of 33, took 0.44127869606018066 seconds\n","finished batch 29 out of 34, took 0.8843541145324707 seconds\n","finished batch 30 out of 34, took 0.4508635997772217 seconds\n","finished batch 31 out of 35, took 0.8944652080535889 seconds\n","finished batch 32 out of 36, took 1.243286371231079 seconds\n","finished batch 33 out of 36, took 0.43082475662231445 seconds\n","finished batch 34 out of 37, took 0.8737161159515381 seconds\n","finished batch 35 out of 38, took 1.3300034999847412 seconds\n","finished batch 36 out of 39, took 1.683852195739746 seconds\n","finished batch 37 out of 39, took 0.435915470123291 seconds\n","finished batch 38 out of 40, took 0.8779518604278564 seconds\n","finished batch 39 out of 41, took 1.320868730545044 seconds\n","finished batch 40 out of 42, took 1.6679871082305908 seconds\n","finished batch 41 out of 42, took 0.4358072280883789 seconds\n","finished batch 42 out of 43, took 0.881089448928833 seconds\n","finished batch 43 out of 44, took 1.2894997596740723 seconds\n","finished batch 44 out of 44, took 0.4483635425567627 seconds\n","finished batch 45 out of 45, took 0.8647117614746094 seconds\n","computing total accuracy\n","total correct: 0, total found: 19106\n","total acc: 0.0\n","Completed, takes: 19.862764358520508 seconds\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ooJwqE_4GABs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f7631b52-a55b-4a67-94a1-87a613c91bba","executionInfo":{"status":"ok","timestamp":1583084477390,"user_tz":0,"elapsed":568,"user":{"displayName":"Aneesh Pappu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAd7g9Mtf5hzYf5M_-AuNqtCn2V_uL1Ys5IGMC-sA=s64","userId":"12681851174811573325"}}},"source":["with open('../data/scores/{}/{}.txt'.format(EXPERIMENT_FOLDER_NAME,'analogies_' + str(datetime.now())), 'w') as out:\n","  out.write(\"Total accuracy: {}. Total correct {} out of total found {}\".format(total_accuracy, total_correct, total_found))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["0.0\n"],"name":"stdout"}]}]}